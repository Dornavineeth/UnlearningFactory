model:
  model_args:
    pretrained_model_name_or_path: meta-llama/Llama-3.1-8B-Instruct
    attn_implementation: flash_attention_2
    torch_dtype: bfloat16
  tokenizer_args:
    pretrained_model_name_or_path: meta-llama/Llama-3.1-8B-Instruct
  template_args:
    apply_chat_template: false
    user_start_tag: '<|start_header_id|>system<|end_header_id|>


      You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>


      '
    user_end_tag: <|eot_id|>
    asst_tag: '<|start_header_id|>assistant<|end_header_id|>


      '
eval:
  data:
    TOFU_QA_FORGET10:
      args:
        subset: forget10
        split: train
        path: locuslab/TOFU
        question_key: question
        answer_key: answer
        max_length: 512
    TOFU_QA_FORGET10_P:
      args:
        subset: forget10_perturbed
        split: train
        path: locuslab/TOFU
        question_key: paraphrased_question
        answer_key: paraphrased_answer
        max_length: 512
  collator:
    DataCollatorForSupervisedDatasetWithIndex:
      args: {}
  name: tofu
  metrics_cfg: {}
  batch_size: 16
output_dir: evals
