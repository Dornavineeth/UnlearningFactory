model_args:
  pretrained_model_name_or_path: meta-llama/Llama-2-7b-chat-hf
  attn_implementation: 'flash_attention_2'
  torch_dtype: bfloat16
tokenizer_args:
  pretrained_model_name_or_path: meta-llama/Llama-2-7b-chat-hf
template_args: # Used in creating prompts for the dataset. See src/data/utils.py#preprocess_chat_instance.
# following https://www.reddit.com/r/LocalLLaMA/comments/1561vn5/here_is_a_practical_multiturn_llama2chat_prompt/
  apply_chat_template: False
  user_start_tag: "[INST] "
  user_end_tag: " [/INST]"
  asst_start_tag: "" # Abusing prompt format of llama2 which has " ", As having this leads to issue of seperating prompt and label tokens
  asst_end_tag: ""